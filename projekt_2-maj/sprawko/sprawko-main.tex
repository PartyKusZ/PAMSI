\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[polish]{babel}
\usepackage{geometry}
\usepackage{tabularx}
\usepackage[table,xcdraw]{xcolor}
\usepackage{color}
\usepackage{subfig}
\usepackage{sidecap}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{multirow}
\setlength{\parindent}{0pt}
\usepackage{hyperref}
\usepackage{titlesec}
\titlelabel{\thetitle.\quad}
\usepackage{amsmath}
\usepackage{anyfontsize}
\usepackage{indentfirst}
\usepackage{listings}
\usepackage{multicol}
\usepackage{pgfplots}
\usepackage{fancyhdr}

\pgfplotsset{%
compat=newest,%
/pgf/number format/use comma,%
/pgf/number format/1000 sep={\,},%
/pgf/number format/min exponent for 1000 sep=4,}


\definecolor{clr-background}{RGB}{255,255,255}
\definecolor{clr-text}{RGB}{0,0,0}
\definecolor{clr-string}{RGB}{163,21,21}
\definecolor{clr-namespace}{RGB}{0,0,0}
\definecolor{clr-preprocessor}{RGB}{128,128,128}
\definecolor{clr-keyword}{RGB}{0,0,255}
\definecolor{clr-type}{RGB}{59, 112, 230}
\definecolor{clr-variable}{RGB}{0,0,0}
\definecolor{clr-constant}{RGB}{111,0,138} % macro color
\definecolor{clr-comment}{RGB}{0,128,0}
\definecolor{mycolor}{rgb}{0.8,0.8,0.8}
\lstset{
  xleftmargin=20pt,
  xrightmargin=0pt,
  framexleftmargin=20pt,
  framexrightmargin=0pt,
  framexbottommargin=2pt,
  columns=flexible,
  keepspaces=true,
  showstringspaces=false,
  backgroundcolor=\color{clr-background},
  basicstyle=\color{clr-text}, % any text
  stringstyle=\color{clr-string},
  identifierstyle=\color{clr-variable}, % just about anything that isn't a directive, comment, string or known type
  commentstyle=\color{clr-comment},
  keywordstyle=\color{clr-type},
  tabsize=4,
  aboveskip=1em,
  belowskip=0em,
  frame=b,
  rulecolor=\color{mycolor},
  numbers=left,
  numbersep=10pt,
  numberstyle={\fontsize{9pt}{11pt}\selectfont\color{gray}},
}



\newgeometry{tmargin=1.8cm,bmargin=1.8cm,lmargin =1.8cm,rmargin=1.8cm}
\pagestyle{fancy}
\fancyhf{}
\rhead{\textit{Sortowanie}}
\lhead{\textit{Jakub Kusz}}
\cfoot{ \thepage}
\begin{document}

\input{strona_tytulowa.tex}
    
\tableofcontents
\newpage
\section{Cel projektu}
Celem projektu jest zapoznanie się z różnymi algorytmami sortującymi i ich złożonością obliczeniową zależną od różnych zestawów danych. 
\section{Zadanie do wykonania}
Dla danych w pliku projekt2\_dane.csv należy wykonać eksperymenty z sortowaniem danych względem
rankingu filmów. Załączony plik jest okrojoną bazą filmów ,,IMDb Largest Review Dataset'' z kaggle.com.
Plik zawiera tylko tytuł oraz ranking. Proszę o wykonanie następujących zadań:
\begin{enumerate}
    \item Przefiltrowanie danych i usunięcie pustych wpisów w polu ranking (jeśli występują). Proszę zmierzyć
    i podać w sprawozdaniu czas przeszukiwania. Czy był on zgodny z oczekiwaną złożonością przeszukiwania
    dla wybranej struktury danych?
    \item Przygotować strukturę danych zawierającą odpowiednio: 10 000, 100 000, 500 000, 1 000 000,
    maksymalną ilość danych z pliku.
    \item Przeprowadzić analizę efektywności sortowania na danych z §2 z wykorzystaniem zaimplementowanych
    algorytmów. 
    \item  Dodatkowo dla każdego zestawu danych proszę podać w tabeli czas sortowania, średnią wartość
    oraz medianę rankingu.
    
\end{enumerate}
Zostały przeze mnie wybrane 4 algorytmy sortujące, które poddałem testom:

\begin{enumerate}
    \item \textbf{Quicksort},
    \item \textbf{Mergesort},
    \item \textbf{Bucketsort},
    \item \textbf{Introsort}.
\end{enumerate}

Kod źródłowy programu znajduje się \href{https://github.com/PartyKusZ/PAMSI/tree/main/projekt_2-maj/src}{\textbf{Pod tym adresem}}
\section{Filtrowanie danych}\label{ch: filtorwanie}
Pierwszym krokiem było przefiltrowanie danych z pliku .csv zawierającego tytułu i rankingi. Należało usunąć wpisy, 
które nie zawierały rankingu. Filtrowanie odbywało się jednocześnie z pobieraniem poszczególnych linii z pliku .csv - jeśli dana 
linia nie zawierała pola z rankingiem to nie zostawała zapisana do struktury danych. 
\subsection{Przewidywana złożoność obliczeniowa}
Przewiduje sie złożoność obliczeniową liniową. Złożoność dodania na koniec wykorzystanego w zadaniu kontenera std :: vector 
jest stała w czasie, sprawdzenie czy dana linia zawiera pole z rankingiem również, wiec czas przefiltrowania danych zależny jest 
tylko od ich ilości, więc powinien być liniowy. W notacji dużego O:

{\Large \begin{equation*}
       O(n)
\end{equation*}}
\subsection{Wyznaczona złożoność obliczeniowa}
W celu wyznaczenia złożoności obliczeniowej został przeprowadzony test, polegający na mierzeniu czasu wczytywania 
liczby kolejnych krotności 1000 linii z pliku .csv.
\input{wykres_filtrowanie.tex}
\section{Quicksort}\label{ch: quicksort}
Algorytm wykorzystuje technikę "dziel i zwyciężaj". Według ustalonego schematu wybierany jest jeden element w 
sortowanej tablicy, który będziemy nazywać pivot. Pivot może być elementem środkowym, pierwszym, ostatnim, losowym 
lub wybranym według jakiegoś innego schematu dostosowanego do zbioru danych. Następnie ustawiamy elementy nie 
większe na lewo tej wartości, natomiast nie mniejsze na prawo. W ten sposób powstaną nam dwie części 
tablicy (niekoniecznie równe), gdzie w pierwszej części znajdują się elementy nie większe od drugiej. 
Następnie każdą z tych podtablic sortujemy osobno według tego samego schematu. 
\subsection{Złożoność obliczenia}
W zależności od rozkładu danych i elementu pivot  określa się następujące złożoności obliczeniowe:
\subsubsection{Przypadek optymistyczny}\label{ch: q optymistyczny}
W przypadku optymistycznym, jeśli mamy szczęście za każdym razem wybrać medianę z sortowanego fragmentu tablicy, to liczba porównań niezbędnych do uporządkowania n-elementowej tablicy opisana jest rekurencyjnym wzorem:

{ \Large \begin{equation*}
       O(n \cdot log_2 n)  
\end{equation*}}
\subsubsection{Przypadek przeciętny}\label{ch: q przecietny}
W przypadku przeciętnym, to jest dla równomiernego rozkładu prawdopodobieństwa wyboru elementu z tablicy: 


{ \Large \begin{equation*}
       O(1,39 \cdot n \cdot log_2 n )  
\end{equation*}}

\subsubsection{Przypadek pesymistyczny}
W przypadku pesymistycznym, jeśli zawsze wybierzemy element najmniejszy (albo największy) w sortowanym fragmencie tablicy, to: 


{\Large \begin{equation*}
       O\left({n^2}\right)  
\end{equation*}}

\subsection{Implementacja}
Poniższy listing przedstawia rekurencyjną implementację algorytmu Quicksort. 

{\small \begin{lstlisting}[language=C++]
void quick_sort(double *tab, int left, int right){
	if(right <= left) 
              return;
	
	int i = left - 1;
    int j = right + 1; 
    int pivot = tab[(left+right)/2]; 
	
	while(1){
		
		while(pivot>tab[++i]);
		while(pivot<tab[--j]);
		
		
		if( i <= j)
			std :: swap(tab[i],tab[j]);
		else
			break;
	}

	if(j > left)
	    quick_sort(tab, left, j);
	if(i < right)
	    quick_sort(tab, i, right);
}
\end{lstlisting}
}

\subsection{Testy złożoności obliczeniowej}
W celu eksperymentalnego wyznaczenia złożoności obliczeniowej algorytmu Quicksort zostały przeprowadzone testy szybkości dla 
czterech różniących się ilością danych pakietów: 
\begin{itemize}
       \item \textbf{10000},
       \item  \textbf{100000},
       \item \textbf{500000},
       \item \textbf{962903}.
\end{itemize}
Dla każdego zestawu danych przeprowadzono 100 pomiarów czasu. Najmniejszą paczką dla poszczególnej ilości danych jest 
$\frac{x}{100}$ gdzie x to ilość danych. Każdy kolejny pomiar pomiar wykonywano dla $n \cdot \frac{x}{100}$ ilości w paczce,
gdzie $n \in 1,2,3 ... 100$.
Na przykład dla \textbf{10000} poszczególne paczki to 100, 200, 300... a dla \textbf{500000} to 5000, 10000, 15000... .

\begin{figure}[H]
       \centering
       \subfloat[ilość danych: 10000]{\input{wykres_qs_10'000.tex}}
       \quad
       \subfloat[ilość danych: 100000]{\input{wykres_qs_100'000.tex}}
\end{figure}
\begin{figure}[H]\ContinuedFloat
       \centering
       
       \subfloat[ilość danych: 500000]{\input{wykres_qs_500'000.tex}}
       \quad
       \subfloat[ilość danych: 962903]{\input{wykres_qs_max.tex}}
       \renewcommand{\figurename}{Wykres.}

       \caption{Wykresy przedstawiające czas sortowania określonej ilości danych}
       \label{fig: qsort}
       \phantomcaption
\end{figure}

Na Rys. \ref{fig: qsort} można zauważyć, iż złożoność obliczeniowa prezentuje się jako f. liniowa,
co nie jest dużym zaskoczeniem, ponieważ złożoność zarówno optymistyczna(\ref{ch: q optymistyczny}) jak i przeciętna(\ref{ch: q przecietny}) w dużej dziedzinie kształtem 
również przypominają f. linowe, aczkolwiek nimi nie są.

\section{Mergesort}\label{ch: mergesort}




Ideą działania algorytmu jest dzielenie zbioru danych na mniejsze zbiory, aż do uzyskania n zbiorów jednoelementowych, które same
z siebie są posortowane, następnie zbiory te są łączone w coraz większe zbiory posortowane, aż do
uzyskania jednego, posortowanego zbioru n-elementowego. Etap dzielenia nie jest skomplikowany,
dzielenie następuje bez sprawdzania jakichkolwiek warunków. Z kolei łączenie zbiorów
posortowanych wymaga odpowiedniego wybierania poszczególnych elementów z łączonych zbiorów z
uwzględnieniem faktu, że wielkość zbioru nie musi być równa (parzysta i nieparzysta ilość elementów),
oraz tego, iż wybieranie elementów z poszczególnych zbiorów nie musi następować naprzemiennie,
przez co jeden zbiór może osiągąć swój koniec wcześniej niż drugi. Robi sie to w następujący sposób.
Kopiujemy zawartość zbioru głównego do struktury pomocniczej. Następnie, operując wyłącznie
na kopii, ustawiamy wskaźniki na początki kolejnych zbiorów i porównujemy wskazywane wartości.
Mniejszą wartość wpisujemy do zbioru głównego i przesuwamy odpowiedni wskaźnik o 1 i czynności
powtarzamy, aż do momentu, gdy jeden ze wskaźników osiągnie koniec zbioru. Wówczam mamy do
rozpatrzenia dwa przypadki, gdy zbiór 1 osiągnął koniec i gdy zbiór 2 osiągnął koniec. W przypadku
pierwszym nie będzie problemu, elementy w zbiorze głównym są już posortowane i ułożone na właściwych miejscach. W przypadku drugim trzeba skopiować pozostałe elementy zbioru pierwszego
pokolei na koniec. Po zakończeniu wszystkich operacji otrzmujemy posortowany zbiór główny.
\newpage
\subsection{Złożoność obliczeniowa}
Złożoność obliczeniowa jest niezależna od zestawu danych:

{ \Large \begin{equation*}
       O(n \cdot log_2 n)  
\end{equation*}}
\subsection{Implementacja}
Poniższe listingi przedstawiają funkcje funkcję scalającą i funkcje sortującą:
\subsubsection{Scalanie}
{\small
\begin{lstlisting}[language=C++]
void merge(double array [], int const left , int const mid, int const right ){
    int const sub_array_one = mid - left + 1;
    int const sub_array_two = right - mid;
    double *left_array = new double[sub_array_one];
    double *right_array = new double[sub_array_two];
    for ( int i = 0; i < sub_array_one; i++)
        left_array [ i ] = array[ left + i ];
    for ( int j = 0; j < sub_array_two; j++)
         right_array[ j ] = array[mid + 1 + j];
    int index_of_sub_array_one = 0;
    int index_of_sub_array_two = 0;
    int index_of_merged_array = left;
    while (index_of_sub_array_one < sub_array_one && index_of_sub_array_two < sub_array_two) {
        if (left_array [index_of_sub_array_one] <= right_array[index_of_sub_array_two]) {
            array[index_of_merged_array] = left_array[index_of_sub_array_one];
            index_of_sub_array_one++;
        }
        else {
            array[index_of_merged_array] = right_array[index_of_sub_array_two];
            index_of_sub_array_two++;
        }
        index_of_merged_array++;
    }
     while (index_of_sub_array_one < sub_array_one) {
        array[index_of_merged_array] = left_array[index_of_sub_array_one];
        index_of_sub_array_one++;
        index_of_merged_array++;
    }
    while (index_of_sub_array_two < sub_array_two) {
        array[index_of_merged_array] = right_array[index_of_sub_array_two];
        index_of_sub_array_two++;
        index_of_merged_array++;
    }
 }
       \end{lstlisting}

}

\subsubsection{Sortowanie}

{\small
\begin{lstlisting}[language=C++]

       void merge_sort(double *arr, int left , int right ){
              if ( left >= right)
                  return;
          
              auto mid = left + (right - left ) / 2;
              merge_sort(arr, left , mid);
              merge_sort(arr, mid + 1, right);
              merge(arr, left , mid, right );
           }
       \end{lstlisting}

}

\subsection{Testy złożoności obliczeniowej}
W celu eksperymentalnego wyznaczenia złożoności obliczeniowej algorytmu Mergesort zostały przeprowadzone testy szybkości dla 
czterech różniących się ilością danych pakietów: 
\begin{itemize}
       \item \textbf{10000},
       \item  \textbf{100000},
       \item \textbf{500000},
       \item \textbf{962903}.
\end{itemize}
Dla każdego zestawu danych przeprowadzono 100 pomiarów czasu. Najmniejszą paczką dla poszczególnej ilości danych jest 
$\frac{x}{100}$ gdzie x to ilość danych. Każdy kolejny pomiar pomiar wykonywano dla $n \cdot \frac{x}{100}$ ilości w paczce,
gdzie $n \in 1,2,3 ... 100$.
Na przykład dla \textbf{10000} poszczególne paczki to 100, 200, 300... a dla \textbf{500000} to 5000, 10000, 15000... .

\begin{figure}[H]
       \centering
       \subfloat[ilość danych: 10000]{\input{wykres_ms_10'000.tex}}
       \quad
       \subfloat[ilość danych: 100000]{\input{wykres_ms_100'000.tex}}
\end{figure}
\begin{figure}[H]\ContinuedFloat
       \centering
       
       \subfloat[ilość danych: 500000]{\input{wykres_ms_500'000.tex}}
       \quad
       \subfloat[ilość danych: 962903]{\input{wykres_ms_max.tex}}
       \renewcommand{\figurename}{Wykres.}
       \caption{Wykresy przedstawiające czas sortowania określonej ilości danych}
       \label{fig: mergesort}
       \phantomcaption
\end{figure}


\section{Introsort}\label{ch: introsort}
Sortowanie introspektywne to odmiana sortowania hybrydowego, w której wyeliminowany został
problem złożoności $O(n^2)$ występującej w najgorszym przypadku algorytmu sortowania szybkiego.
Algorytm łączy w sobie 3 rodzaje sortowań, które wywoływane są dla różnych przypadków.
Używa on sortowania przez wstawianie (insertion sort), sortowania przez kopcowanie (heap sort)
i sortowania szybkiego (quick sort).
\subsection{Złożoność obliczeniowa}
\subsubsection{Przypadek optymistyzcny}

{ \Large \begin{equation*}
       O(n \cdot \log_2 n)  
\end{equation*}}
\subsubsection{Przypadek przeciętny}

{ \Large \begin{equation*}
       O( 1.39 \cdot n \cdot log_2 n)   
\end{equation*}}

\subsubsection{Przypadek pesymistyczny}

{ \Large \begin{equation*}
       O(2 \cdot n \cdot log_2 n)   
\end{equation*}}

\subsection{Implementacja}

Poniższe listingi przedstawiają poszczególne funkcje konieczne do wykonania Introsort

\subsubsection{Podział}

{\small
\begin{lstlisting}[language=C++]
       int partition (double *data, int left , int right ) {
              int pivot = data[right ];
              int temp;
              int i = left ;
              for ( int j = left ; j < right; ++j){
                  if (data[j ] <= pivot){
                      temp = data[j];
                      data[j ] = data[i ];
                      data[i ] = temp;
                      i++;
                  }
              }
          
               data[right ] = data[i ];
              data[i ] = pivot;
          
              return i;
          }
       \end{lstlisting}

}

\subsubsection{Sortowanie}

{\small
\begin{lstlisting}[language=C++]
       void intro_sort(double *arr , int size ) {
              int partitionSize = partition(arr , 0, size - 1);
                  if ( partitionSize < 16){
                      insertion_sort(arr , size );
                  }
                  else if ( partitionSize >(2 * std :: log( size ))){
                      heap_sort(arr, size );
                  }
                  else{
                      quick_sort(arr, 0, size -1);
                  }
          }
       \end{lstlisting}

}

\subsection{Testy złożoności obliczeniowej}
W celu eksperymentalnego wyznaczenia złożoności obliczeniowej algorytmu Introsort zostały przeprowadzone testy szybkości dla 
czterech różniących się ilością danych pakietów: 
\begin{itemize}
       \item \textbf{10000},
       \item  \textbf{100000},
       \item \textbf{500000},
       \item \textbf{962903}.
\end{itemize}
Dla każdego zestawu danych przeprowadzono 100 pomiarów czasu. Najmniejszą paczką dla poszczególnej ilości danych jest 
$\frac{x}{100}$ gdzie x to ilość danych. Każdy kolejny pomiar pomiar wykonywano dla $n \cdot \frac{x}{100}$ ilości w paczce,
gdzie $n \in 1,2,3 ... 100$.
Na przykład dla \textbf{10000} poszczególne paczki to 100, 200, 300... a dla \textbf{500000} to 5000, 10000, 15000... .


\begin{figure}[H]
       \centering
       \subfloat[ilość danych: 10000]{\input{wykres_is_10'000.tex}}
       \quad
       \subfloat[ilość danych: 100000]{\input{wykres_is_100'000.tex}}
\end{figure}
\begin{figure}[H]\ContinuedFloat
       \centering
       \subfloat[ilość danych: 500000]{\input{wykres_is_500'000.tex}}
       \quad
       \subfloat[ilość danych: 962903]{\input{wykres_is_max.tex}}
       \renewcommand{\figurename}{Wykres.}
       \caption{Wykresy przedstawiające czas sortowania określonej ilości danych}
       \label{fig: introsort}
       \phantomcaption
\end{figure}

\section{Dyskusja}

\subsection{Czasy trwania filtracji}

Wyznaczona doświadczalnie złożoność obliczeniowa filtrowania danych (Wykres \ref{fig: filtrowanie}) jest 
zgodna ze złożonością prognozowaną (pkt. \ref{ch: filtorwanie}).\\

Filtrowanie danych trwało $\sim $: \textbf{311 ms}
\subsection{Sortowanie}
\subsubsection{Quicksort}
Analizując Wykres \ref{fig: qsort} można dojść do wniosku, iż złożoność obliczeniowa algorytmu Quicksort jest
liniowa, co nie zgadza się z optymistyczną przeciętną jak i pesymistyczną złożonością. (pkt. \ref{ch: quicksort}). Można 
jednak taki rezultat uznać za właściwy, ponieważ przebieg funkcji $n \cdot log_2 n$ jest kształtem zbliżony do funkcji 
liniowej.\\

Sortowanie największego zestawu danych (962903 elementów) trwało $\sim$: \textbf{88,101 ms}

\subsubsection{Mergesort}


Analizując Wykres \ref{fig: mergesort} można dojść do wniosku, iż złożoność obliczeniowa algorytmu Mergesort jest
liniowa, co nie zgadza się z teoretyczną złożonością. (pkt. \ref{ch: mergesort}). Można 
jednak taki rezultat uznać za właściwy, ponieważ przebieg funkcji $n \cdot log_2 n$ jest kształtem zbliżony do funkcji 
liniowej. Dla największego zestawu powyżej $9 \cdot 10^5$ ilości danych czas wykonywania sortowania gwałtownie wzrasta,
przez co finalny czas sortowania znacznie się wydłużył. \\

Sortowanie największego zestawu danych (962903 elementów) trwało $\sim$: \textbf{442,203 ms}

\subsubsection{Introsort}
Analizując Wykres \ref{fig: introsort} można dojść do wniosku, iż złożoność obliczeniowa algorytmu Introsort jest
liniowa, co nie zgadza się z optymistyczną, przeciętną jak i pesymistyczną złożonością. (pkt. \ref{ch: introsort}). Można 
jednak taki rezultat uznać za właściwy, ponieważ przebieg funkcji $n \cdot log_2 n$ jest kształtem zbliżony do funkcji 
liniowej.\\

Sortowanie największego zestawu danych (962903 elementów) trwało $\sim$: \textbf{232,851 ms}

\section{Podsumowanie}
\subsection{Czas trwania}
Poniższa tabela przedstawia czasy trwania sortowania poszczególnych algorytmów:

\begin{table}[H]
       \centering
       \caption{Zawierająca czasy trwania sortowania poszczególnych algorytmów}
       \begin{tabular}{|c|c|}
       \hline
       algorytm  & czas trwania {[}ms{]} \\ \hline
       Quciksort & 88,101               \\ \hline
       Mergesort & 442,203               \\ \hline
       Introsort & 232,851               \\ \hline
       filtrowanie &    311             \\ \hline
       \end{tabular}
       \label{tab: czasy}
       \end{table}

       \begin{figure}[H]
              \centering
              \begin{tikzpicture}
                     \begin{axis}[
                     width={\textwidth},
                     height={0.4*\textheight},
                     title={Czas sortowania maksymalnej ilości danych dla poszczególnycyh algorytmów},
                     title style={yshift=0.3cm},
                     ylabel={czas [ms]},
                     xlabel={ilość elementów},
                     xmin=0,xmax=1000000 ,
                     ymin=0,ymax=510000,
                     legend pos=north west,
                     ymajorgrids=true,xmajorgrids=true,grid style=dashed
                     ]
                 
                   
                     \addplot[color=red, mark=*,mark size=0.04cm,style=dashed] table {dane/merge962'903.txt};
                     \addlegendentry{Mergesort}
                     \addplot[color=blue, mark=*,mark size=0.04cm,style=dashed] table {dane/qsort692'903.txt};
                     \addlegendentry{Quicksort}
                     \addplot[color=green, mark=*,mark size=0.04cm,style=dashed] table {dane/intro962'903.txt};
                     \addlegendentry{Introsort}

                     
                   
                     \end{axis}
                     \end{tikzpicture}
       \renewcommand{\figurename}{Wykres.}

                     \caption{Przedstawiający czas sortowania maksymalnej ilości danych dla poszczególnycyh algorytmów }
                     \label{fig: final}
              
       \end{figure}

       \subsection{Średnia arytmetyczna i mediana}
       Dla każdego zestawu danych została obliczona średnia arytmetyczna i mediana rankingu. Zostały one zaprezentowane w poniższej 
       tabeli.

       \begin{table}[H]
              \centering
              \caption{Zawierająca wartości średniej arytmetycznej i mediany dla poszczególnych zestawów danych}

              \begin{tabular}{|c|c|c|}
              \hline
              ilość elementów & średnia arytmetyczna & mediana \\ \hline
              10000           & 5.4603                    & 5       \\ \hline
              100000          & 6.08993                    & 7       \\ \hline
              500000          & 6.66572                    & 7       \\ \hline
              962903          & 6.63661                    & 7       \\ \hline
              \end{tabular}
              \end{table}

       \subsection{Wnioski}
       \begin{itemize}
              \item najszybciej działającym algorytmem sortowania okazał się Quicksort (tab. \ref{tab: czasy}),
              \item najwolniej działającym algorytmem sortowania okazał się Introsort (tab. \ref{tab: czasy}),
              \item Mergesort dla danych w ilości większej od ok.  $9 \cdot 10^5$ gwałtowanie zwalnia przez co dla dużej ilości danych okazuje się najwolniejszy, nie udało mi się ustalić tego przyczyny (Wykres \ref{fig: final}),
              \item w przypadku algorytmów sortujących złożoności czasowe wyznaczone doświadczalnie nie są zgodne z teoretycznymi, aczkolwiek na tyle podobne,
              iż wyniki doświadczeń można uznać za satysfakcjonujące.
              \item mimo, iż najszybciej działającym algorytmem jest Quicksort, może on dla niekorzystnego wyznaczenia pivota
              okazać się algorytmem o złożoności kwadratowej (pkt. \ref{ch: quicksort}), w przeciwieństwie do algorytmów Introsort i Mergesort(pkt. \ref{ch: introsort},\ref{ch: mergesort}), które gwarantują taką 
              samą złożoność nie zależnie od zestawu danych, lecz działają wolniej.
       \end{itemize}


      
\end{document}
